














# General
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

# modelling
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer

# Visualization
import seaborn as sns
import matplotlib.pyplot as plt






import warnings
warnings.filterwarnings("ignore")

import sys
sys.path.append('/Users/ayushyapare/Desktop/Ayushya/Snippets')

from DataFrame_Analysis import analyze_dataframe





plt.style.use("ggplot")











# import the dataset
df = pd.read_csv('../data/raw/card_transactions.csv')





# Basic:
# 1. Shape
# 2. Columns - look for artifacts in column name
# 3. Info - look for appropriate datatypes 
# 4. Describe - look for min max mean and std. 


df.shape


df.columns


# some columns have a '.' in the column name (must correct)
# 


df.info()


df.describe()


# Initial observations


# Find duplicate values and remove duplicates
# Find missing values
# Find wrong values like ?, none, -, etc. 
# fill with 'unknown'

#----------------------
# In any column - group some values to new cateories
# 
# ---------------------
# If there is a date time column, then extract the day, month and year separately
# as object datatypes
#----------------------
# Some columns can be dropped
# Some columns have unwanted space ' ' in the name, remove if that is the case
#-----------------------
# After the cleaning save the cleaned dataframe in csv format



# Check for missing values in the DataFrame
df.isnull().sum()


# Display all duplicate rows
# Identify duplicate rows
df.duplicated().sum()






# Remove duplicate(if any)
df_cleaned = df.drop_duplicates(keep = 'first')



# Replace incorrect or placeholder values with 'unknown'
incorrect_values = ['?', 'none', '-', 'N/A', 'nan','NaN', 'NULL']
df_cleaned.replace(incorrect_values, 'unknown', inplace=True)






df_cleaned['min_payments'].fillna(0,inplace=True)


df_cleaned['min_payments']


# Convert the 'datetime_col' to datetime if not already in datetime format
df_cleaned['datetime_col'] = pd.to_datetime(df_cleaned['datetime_col'], errors='coerce')

# Extract day, month, and year as separate columns
df_cleaned['day'] = df_cleaned['datetime_col'].dt.day.astype('str')
df_cleaned['month'] = df_cleaned['datetime_col'].dt.month.astype('str')
df_cleaned['year'] = df_cleaned['datetime_col'].dt.year.astype('str')



# Drop unwanted columns
columns_to_drop = ['col1', 'col2', 'col3'] 
df_cleaned.drop(columns=columns_to_drop, inplace=True)


# Remove leading and trailing spaces from column names
df_cleaned.columns = df_cleaned.columns.str.strip()
df_cleaned.columns


# Save the cleaned DataFrame to a CSV file
df_cleaned.to_csv('../data/processed/adult_cleaned.csv', index=False)


df = df_cleaned


# Advanced (Separate categorical and numerical)
# 1. value counts | Unique values | Missing values
# 2. Explore column of interest
#    1. Hist / Countplot
#    2. Boxplot





from DataFrame_Analysis import eda


eda(df)


X_scaled.isna().sum()


X_scaled.fillna(0,inplace=True)


# Select relevant features
X = df.drop(columns=['customer_id'])

# Normalize numerical features
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Create a DataFrame with the scaled features
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

X_scaled.head()



eda(X_scaled)





from sklearn.cluster import KMeans

# Determine the optimal number of clusters using the elbow method
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

# Plot the elbow method graph
plt.figure()
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Assuming the optimal number of clusters is 4
kmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)
df['KMeans_Cluster'] = kmeans.fit_predict(X_scaled)

df.head()



# K medoids


from sklearn.cluster import KMedoids

# Determine the optimal number of clusters using the elbow method
cost = []
for i in range(1, 11):
    kmedoids = KMedoids(n_clusters=i, random_state=42)
    kmedoids.fit(X_scaled)
    cost.append(kmedoids.inertia_)

# Plot the elbow method graph
plt.figure()
plt.plot(range(1, 11), cost)
plt.title('Elbow Method (K-Medoids)')
plt.xlabel('Number of clusters')
plt.ylabel('Cost')
plt.show()

# Assuming the optimal number of clusters is 4
kmedoids = KMedoids(n_clusters=4, random_state=42)
df['KMedoids_Cluster'] = kmedoids.fit_predict(X_scaled)

print(df.head())



# Hierarchical


from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.cluster import AgglomerativeClustering

# Create the linkage matrix
Z = linkage(X_scaled, method='ward')

# Plot the dendrogram
plt.figure(figsize=(16, 10))
dendrogram(Z)
plt.title('Dendrogram')
plt.xlabel('Customers')
plt.ylabel('Euclidean distances')
plt.show()

# Fit the hierarchical clustering model
hierarchical = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')
df['Hierarchical_Cluster'] = hierarchical.fit_predict(X_scaled)

df.head()



# DBSCAN


from sklearn.cluster import DBSCAN

# Fit the DBSCAN model
dbscan = DBSCAN(eps=0.5, min_samples=5)
df['DBSCAN_Cluster'] = dbscan.fit_predict(X_scaled)

df.head()



# Visualization using PCA


from sklearn.decomposition import PCA

# Perform PCA for dimensionality reduction
pca = PCA(n_components=2)
pca_features = pca.fit_transform(X_scaled)

# Create a DataFrame with PCA components
pca_df = pd.DataFrame(data=pca_features, columns=['PCA1', 'PCA2'])

# Function to plot clusters
def plot_clusters(pca_df, cluster_labels, title):
    pca_df['Cluster'] = cluster_labels
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x='PCA1', y='PCA2', hue='Cluster', data=pca_df, palette='viridis', s=100)
    plt.title(title)
    plt.xlabel('PCA1')
    plt.ylabel('PCA2')
    plt.legend(title='Cluster')
    plt.show()

# Plot clusters for each method
plot_clusters(pca_df, df['KMeans_Cluster'], 'K-Means Clusters (PCA)')
#plot_clusters(pca_df, df['KMedoids_Cluster'], 'K-Medoids Clusters (PCA)')
plot_clusters(pca_df, df['Hierarchical_Cluster'], 'Hierarchical Clusters (PCA)')
plot_clusters(pca_df, df['DBSCAN_Cluster'], 'DBSCAN Clusters (PCA)')






# Silhuette score


from sklearn.metrics import silhouette_score

# Silhouette scores for each method
silhouette_kmeans = silhouette_score(X_scaled, df['KMeans_Cluster'])
#silhouette_kmedoids = silhouette_score(X_scaled, df['KMedoids_Cluster'])
silhouette_hierarchical = silhouette_score(X_scaled, df['Hierarchical_Cluster'])
silhouette_dbscan = silhouette_score(X_scaled, df['DBSCAN_Cluster'])

print(f'Silhouette Score (K-Means): {silhouette_kmeans}')
#print(f'Silhouette Score (K-Medoids): {silhouette_kmedoids}')
print(f'Silhouette Score (Hierarchical): {silhouette_hierarchical}')
print(f'Silhouette Score (DBSCAN): {silhouette_dbscan}')




